{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T15:16:11.709167Z",
     "start_time": "2025-06-01T15:16:11.701595Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Basa",
   "id": "c3ddc19fcc46976c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first dataframe is raw data with annotations and messages mixed together, separated by the tab character '\\t'.",
   "id": "386dcb77fe5e3a04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.073485Z",
     "start_time": "2025-06-01T14:59:34.066023Z"
    }
   },
   "cell_type": "code",
   "source": "df_data_raw = pd.read_csv(\"data/dataset.tsv\", header=None)",
   "id": "769836b9ada9f8b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.088353Z",
     "start_time": "2025-06-01T14:59:34.083110Z"
    }
   },
   "cell_type": "code",
   "source": "df_data_raw.head()",
   "id": "a2b7853601f64ea1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                   0\n",
       "0  ham\\tHave your lunch and come quickly and open...\n",
       "1  spam\\tJoin the UK's horniest Dogging service a...\n",
       "2  spam\\tYOU ARE CHOSEN TO RECEIVE A £350 AWARD! ...\n",
       "3  ham\\talright tyler's got a minor crisis and ha...\n",
       "4  ham\\tSad story of a Man - Last week was my b'd..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham\\tHave your lunch and come quickly and open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam\\tJoin the UK's horniest Dogging service a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam\\tYOU ARE CHOSEN TO RECEIVE A £350 AWARD! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham\\talright tyler's got a minor crisis and ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham\\tSad story of a Man - Last week was my b'd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.123103Z",
     "start_time": "2025-06-01T14:59:34.120425Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Dataset size:\", len(df_data_raw))",
   "id": "7430fb1a78df558a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3715\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's check whether all the lines of the dataset have this formatting:",
   "id": "8c8ec635a8eebd4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.171342Z",
     "start_time": "2025-06-01T14:59:34.167854Z"
    }
   },
   "cell_type": "code",
   "source": "len(df_data_raw) == df_data_raw[0].map(lambda x: x.startswith('ham\\t') or x.startswith('spam\\t')).sum()",
   "id": "373ce2deb8ac5535",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us now parse the dataset automatically in order to separate labels from the messages:",
   "id": "b65d6fe9fbe04b7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.207184Z",
     "start_time": "2025-06-01T14:59:34.198842Z"
    }
   },
   "cell_type": "code",
   "source": "df_data = pd.read_csv(\"data/dataset.tsv\", header=None, sep='\\t', names=['label', 'message_raw'], on_bad_lines='skip')",
   "id": "eb8ffcc9d38bb3ee",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.226968Z",
     "start_time": "2025-06-01T14:59:34.223237Z"
    }
   },
   "cell_type": "code",
   "source": "df_data.head()",
   "id": "6bd795856c5984b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  label                                        message_raw\n",
       "0   ham  Have your lunch and come quickly and open the ...\n",
       "1  spam  Join the UK's horniest Dogging service and u c...\n",
       "2  spam  YOU ARE CHOSEN TO RECEIVE A £350 AWARD! Pls ca...\n",
       "3   ham  alright tyler's got a minor crisis and has to ...\n",
       "4   ham  Sad story of a Man - Last week was my b'day. M..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have your lunch and come quickly and open the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Join the UK's horniest Dogging service and u c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>YOU ARE CHOSEN TO RECEIVE A £350 AWARD! Pls ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>alright tyler's got a minor crisis and has to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sad story of a Man - Last week was my b'day. M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As one can notice, we dropped the 'bad lines' while parsing the data automatically, and it turnes out that there's only one bad line:",
   "id": "a2e820bde41d9c23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.261187Z",
     "start_time": "2025-06-01T14:59:34.259366Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(df_data_raw), len(df_data))",
   "id": "928778d02ee96215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3715 3714\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The reason it was dropped is that it contains another tab character making it impossible for pandas' read_csv to parse it automatically. However, it's just one line, which is negligible compared to our dataset size, so we'll proceed without it:",
   "id": "b802393ed6c963f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.312043Z",
     "start_time": "2025-06-01T14:59:34.309351Z"
    }
   },
   "cell_type": "code",
   "source": "df_data_raw.iloc[801, 0]",
   "id": "221158005d136301",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tHI BABE UAWAKE?FEELLIKW SHIT.JUSTFOUND OUT VIA ALETTER THATMUM GOTMARRIED 4thNOV.BEHIND OURBACKS \\x96 FUCKINNICE!SELFISH\\tDEVIOUSBITCH.ANYWAYI\\x92L CALL U\"\"\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.354793Z",
     "start_time": "2025-06-01T14:59:34.351085Z"
    }
   },
   "cell_type": "code",
   "source": "df_data",
   "id": "2de8f11fb9af09c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     label                                        message_raw\n",
       "0      ham  Have your lunch and come quickly and open the ...\n",
       "1     spam  Join the UK's horniest Dogging service and u c...\n",
       "2     spam  YOU ARE CHOSEN TO RECEIVE A £350 AWARD! Pls ca...\n",
       "3      ham  alright tyler's got a minor crisis and has to ...\n",
       "4      ham  Sad story of a Man - Last week was my b'day. M...\n",
       "...    ...                                                ...\n",
       "3709   ham  Hello which the site to download songs its urg...\n",
       "3710   ham  My planning usually stops at \"find hella weed ...\n",
       "3711   ham                  Are u awake? Is there snow there?\n",
       "3712   ham                              Sorry I'll call later\n",
       "3713   ham                           You can never do NOTHING\n",
       "\n",
       "[3714 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have your lunch and come quickly and open the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Join the UK's horniest Dogging service and u c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>YOU ARE CHOSEN TO RECEIVE A £350 AWARD! Pls ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>alright tyler's got a minor crisis and has to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sad story of a Man - Last week was my b'day. M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello which the site to download songs its urg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3710</th>\n",
       "      <td>ham</td>\n",
       "      <td>My planning usually stops at \"find hella weed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3711</th>\n",
       "      <td>ham</td>\n",
       "      <td>Are u awake? Is there snow there?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>ham</td>\n",
       "      <td>You can never do NOTHING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3714 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us first clean the messages and convert them to lists of words.",
   "id": "bb141725904bed28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.398684Z",
     "start_time": "2025-06-01T14:59:34.396597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def basic_clean(text):\n",
    "    \"\"\"Remove all characters which are not letters\n",
    "    or spaces and make the rest lowercase.\"\"\"\n",
    "    text_cleaned = re.sub(r'[^a-z ]', '', text.lower())\n",
    "    return text_cleaned"
   ],
   "id": "f34ef2bff20c267a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.464395Z",
     "start_time": "2025-06-01T14:59:34.453449Z"
    }
   },
   "cell_type": "code",
   "source": "lists_of_words = df_data['message_raw'].map(basic_clean).str.split()",
   "id": "ded12953916bae8d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.486109Z",
     "start_time": "2025-06-01T14:59:34.482540Z"
    }
   },
   "cell_type": "code",
   "source": "lists_of_words",
   "id": "80d8eb94b5376a06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [have, your, lunch, and, come, quickly, and, o...\n",
       "1       [join, the, uks, horniest, dogging, service, a...\n",
       "2       [you, are, chosen, to, receive, a, award, pls,...\n",
       "3       [alright, tylers, got, a, minor, crisis, and, ...\n",
       "4       [sad, story, of, a, man, last, week, was, my, ...\n",
       "                              ...                        \n",
       "3709    [hello, which, the, site, to, download, songs,...\n",
       "3710    [my, planning, usually, stops, at, find, hella...\n",
       "3711              [are, u, awake, is, there, snow, there]\n",
       "3712                            [sorry, ill, call, later]\n",
       "3713                       [you, can, never, do, nothing]\n",
       "Name: message_raw, Length: 3714, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, let's lemmatize those words and remove stopwords.",
   "id": "2c6461e2cba05d09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:34.531960Z",
     "start_time": "2025-06-01T14:59:34.528998Z"
    }
   },
   "cell_type": "code",
   "source": "stop_words = pd.read_csv(\"data/stop_words.csv\", header=None).squeeze().to_list()",
   "id": "22adc8f0898fba4",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:36.086016Z",
     "start_time": "2025-06-01T14:59:34.560329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_data['words'] = lists_of_words.map(\n",
    "    lambda words: [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    ")"
   ],
   "id": "40f91978005c1519",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:36.104049Z",
     "start_time": "2025-06-01T14:59:36.100629Z"
    }
   },
   "cell_type": "code",
   "source": "df_data['words']",
   "id": "19eea42307f7a383",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      [lunch, come, quickly, open, door]\n",
       "1       [join, uk, horniest, dogging, service, u, sex,...\n",
       "2       [chosen, receive, award, pls, call, claim, num...\n",
       "3       [alright, tyler, got, minor, crisis, home, soo...\n",
       "4       [sad, story, man, last, week, bday, wife, didn...\n",
       "                              ...                        \n",
       "3709           [hello, site, download, song, urgent, pls]\n",
       "3710    [planning, usually, stop, find, hella, weed, s...\n",
       "3711                                     [u, awake, snow]\n",
       "3712                            [sorry, ill, call, later]\n",
       "3713                                     [never, nothing]\n",
       "Name: words, Length: 3714, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's output top-10 most occurring words for ham and spam messages.",
   "id": "901d8719dffd0299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:36.143477Z",
     "start_time": "2025-06-01T14:59:36.140485Z"
    }
   },
   "cell_type": "code",
   "source": "df_data_long = df_data[df_data['words'].map(len) >= 3]",
   "id": "9497c805cf620e07",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:36.176667Z",
     "start_time": "2025-06-01T14:59:36.172861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ham_words = df_data_long.loc[df_data_long['label'] == 'ham', 'words'].to_list()\n",
    "spam_words = df_data_long.loc[df_data_long['label'] == 'spam', 'words'].to_list()\n",
    "\n",
    "ham_words_flattened = [word for message in ham_words for word in message]\n",
    "spam_words_flattened = [word for message in spam_words for word in message]"
   ],
   "id": "9be0e36e6a20c81c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:36.213544Z",
     "start_time": "2025-06-01T14:59:36.202777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fdist_ham = FreqDist(ham_words_flattened)\n",
    "fdist_spam = FreqDist(spam_words_flattened)"
   ],
   "id": "62ffa4c13ace213a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:00:10.993061Z",
     "start_time": "2025-06-01T15:00:10.990309Z"
    }
   },
   "cell_type": "code",
   "source": "fdist_ham.most_common(10)",
   "id": "4b3fa5e45d6abd58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u', 681),\n",
       " ('im', 303),\n",
       " ('get', 195),\n",
       " ('dont', 190),\n",
       " ('go', 170),\n",
       " ('come', 165),\n",
       " ('like', 165),\n",
       " ('know', 163),\n",
       " ('ill', 159),\n",
       " ('ltgt', 157)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T14:59:36.261329Z",
     "start_time": "2025-06-01T14:59:36.258889Z"
    }
   },
   "cell_type": "code",
   "source": "fdist_spam.most_common(10)",
   "id": "71101c4d60a9839e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('call', 243),\n",
       " ('free', 147),\n",
       " ('u', 106),\n",
       " ('text', 94),\n",
       " ('mobile', 92),\n",
       " ('ur', 92),\n",
       " ('txt', 90),\n",
       " ('claim', 75),\n",
       " ('stop', 73),\n",
       " ('reply', 69)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# xHardcorex",
   "id": "d2807773384f5cd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:18:48.844848Z",
     "start_time": "2025-06-01T15:18:48.808292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = df_data['words'].to_list()\n",
    "all_words_flattened = [word for message in all_words for word in message]\n",
    "fdist_all = FreqDist(all_words_flattened)\n",
    "frequent_long_words = sorted([word for word, count in fdist_all.most_common() if count >= 10 and len(word) >= 3])"
   ],
   "id": "64ec3d0f831728bf",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:43:33.404977Z",
     "start_time": "2025-06-01T15:43:33.399983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def frequent_words_counter(words, vocabulary):\n",
    "    counter = Counter(w for w in words if w in vocabulary)\n",
    "    return [counter[word] for word in words]"
   ],
   "id": "a9430995ee1e374f",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:43:34.750635Z",
     "start_time": "2025-06-01T15:43:34.627294Z"
    }
   },
   "cell_type": "code",
   "source": "df_data['popular_word_frequency'] = df_data['words'].map(lambda words: frequent_words_counter(words, vocabulary=frequent_long_words))",
   "id": "36f351ababd0e410",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:43:35.968491Z",
     "start_time": "2025-06-01T15:43:35.953302Z"
    }
   },
   "cell_type": "code",
   "source": "df_data[['words', 'popular_word_frequency']]",
   "id": "418aa7545fe7775a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  words  \\\n",
       "0                    [lunch, come, quickly, open, door]   \n",
       "1     [join, uk, horniest, dogging, service, u, sex,...   \n",
       "2     [chosen, receive, award, pls, call, claim, num...   \n",
       "3     [alright, tyler, got, minor, crisis, home, soo...   \n",
       "4     [sad, story, man, last, week, bday, wife, didn...   \n",
       "...                                                 ...   \n",
       "3709         [hello, site, download, song, urgent, pls]   \n",
       "3710  [planning, usually, stop, find, hella, weed, s...   \n",
       "3711                                   [u, awake, snow]   \n",
       "3712                          [sorry, ill, call, later]   \n",
       "3713                                   [never, nothing]   \n",
       "\n",
       "                                 popular_word_frequency  \n",
       "0                                       [1, 1, 0, 1, 1]  \n",
       "1            [1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]  \n",
       "2            [0, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1, 1]  \n",
       "3                           [1, 0, 1, 0, 0, 1, 0, 1, 0]  \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, ...  \n",
       "...                                                 ...  \n",
       "3709                                 [1, 0, 1, 1, 1, 1]  \n",
       "3710                        [0, 0, 1, 1, 0, 0, 1, 0, 0]  \n",
       "3711                                          [0, 0, 1]  \n",
       "3712                                       [1, 1, 1, 1]  \n",
       "3713                                             [1, 1]  \n",
       "\n",
       "[3714 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>popular_word_frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[lunch, come, quickly, open, door]</td>\n",
       "      <td>[1, 1, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[join, uk, horniest, dogging, service, u, sex,...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[chosen, receive, award, pls, call, claim, num...</td>\n",
       "      <td>[0, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[alright, tyler, got, minor, crisis, home, soo...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[sad, story, man, last, week, bday, wife, didn...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>[hello, site, download, song, urgent, pls]</td>\n",
       "      <td>[1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3710</th>\n",
       "      <td>[planning, usually, stop, find, hella, weed, s...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3711</th>\n",
       "      <td>[u, awake, snow]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>[sorry, ill, call, later]</td>\n",
       "      <td>[1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>[never, nothing]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3714 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T15:43:38.070113Z",
     "start_time": "2025-06-01T15:43:37.767893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def word_frequency_vector(tokens, vocab):\n",
    "    fdist = FreqDist(tokens)\n",
    "    total = len(tokens)\n",
    "    return [fdist[word] / total if total > 0 else 0.0 for word in vocab]\n",
    "\n",
    "df_data['frequency_vector'] = df_data['words'].map(lambda words: word_frequency_vector(words, vocab=frequent_long_words))"
   ],
   "id": "cac99bbc6d6db2dc",
   "outputs": [],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
